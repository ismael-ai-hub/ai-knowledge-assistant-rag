\# AI Knowledge Assistant â€“ Local RAG System (Ollama + LangChain)



This project implements a \*\*100% local, cost-free Retrieval-Augmented Generation (RAG) system\*\* that allows users to query their own documents (PDFs) and receive \*\*grounded, source-backed answers\*\*.



It is designed to reflect \*\*enterprise-grade AI agent architectures\*\*, with a focus on reliability, transparency, and data privacy.



---



\## ğŸš€ Key Features



\- ğŸ”’ \*\*Fully local inference\*\* (no external APIs, no data leakage)

\- ğŸ¤– LLM-powered question answering using \*\*Ollama\*\*

\- ğŸ“š Retrieval-Augmented Generation (RAG) with \*\*source attribution\*\*

\- ğŸ§  Semantic search with \*\*ChromaDB\*\*

\- ğŸ“„ Multi-PDF ingestion pipeline

\- ğŸŒ REST API exposed via \*\*FastAPI\*\*

\- ğŸ§ª Interactive CLI + API usage

\- ğŸ—ï¸ Clean, modular, production-oriented codebase



---



\## ğŸ§  Architecture Overview



```text

PDF Documents

&nbsp;    â†“

Text Splitting (LangChain)

&nbsp;    â†“

Embeddings (Hugging Face)

&nbsp;    â†“

Vector Store (ChromaDB)

&nbsp;    â†“

Retriever

&nbsp;    â†“

LLM (Ollama â€“ Mistral / LLaMA)

&nbsp;    â†“

Answer + Sources



